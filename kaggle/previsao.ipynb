{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "115bc7d7",
   "metadata": {},
   "source": [
    "### Previsão de Sucesso de Startups \n",
    "## Objetivo e Estratégia\n",
    "O objetivo é desenvolver um modelo preditivo robusto para classificar startups como sucesso (1) ou insucesso (0), buscando a acurácia máxima.\n",
    "\n",
    "A estratégia final é um Modelo Dominante Único HistGradientBoostingClassifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a91795",
   "metadata": {},
   "source": [
    "## 1. Configuração e Funções Essenciais\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c514c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importações necessárias e configuração\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score \n",
    "from sklearn.ensemble import HistGradientBoostingClassifier \n",
    "from sklearn.base import clone \n",
    "from sklearn.feature_selection import SelectKBest, f_classif \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 75)\n",
    "print(\"INÍCIO DO PIPELINE: HISTGRADIENTBOOSTING DOMINANTE E FEATURE SELECTION\")\n",
    "print(\"=\" * 75)\n",
    "\n",
    "# Carregamento dos dados\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "TARGET = 'labels'\n",
    "\n",
    "# Parâmetros de estabilidade\n",
    "CLASS_WEIGHT_RATIO = {0: 1.0, 1: train[TARGET].value_counts()[0]/train[TARGET].value_counts()[1]}\n",
    "SKF = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n",
    "\n",
    "def create_simple_features(df):\n",
    "    df = df.copy()\n",
    "    required_risk_cols = ['is_cleantech', 'is_hardware']\n",
    "    for col in required_risk_cols:\n",
    "        if col not in df.columns: df[col] = 0\n",
    "    age_funding_cols = ['age_first_funding_year', 'age_last_funding_year']\n",
    "    age_milestone_cols = ['age_first_milestone_year', 'age_last_milestone_year']\n",
    "    \n",
    "    for col in age_funding_cols + age_milestone_cols: df[f'is_nan_{col}'] = df[col].isna().astype(int)\n",
    "    df['is_nan_funding_total_usd'] = df['funding_total_usd'].isna().astype(int)\n",
    "    \n",
    "    df['log_funding'] = np.log1p(df['funding_total_usd'].fillna(0))\n",
    "    df['log_relationships'] = np.log1p(df['relationships'])\n",
    "    df['funding_efficiency'] = df['funding_total_usd'] / (df['funding_rounds'] + 1)\n",
    "    df['funding_time_span'] = (df['age_last_funding_year'] - df['age_first_funding_year']).fillna(0)\n",
    "    df['investment_diversity'] = df[['has_VC', 'has_angel', 'has_roundA', 'has_roundB', 'has_roundC', 'has_roundD']].sum(axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Função OOF (Válida para HGB e Modelos Baseados em Árvore)\n",
    "def generate_oof_predictions(model, X_train, y_train, X_test, folds):\n",
    "    oof_proba = np.zeros(X_train.shape[0])\n",
    "    test_preds = np.zeros(X_test.shape[0])\n",
    "    y_train_arr = y_train.values if isinstance(y_train, pd.Series) else y_train\n",
    "    total_splits = folds.get_n_splits(X_train, y_train_arr)\n",
    "    \n",
    "    for fold_idx, (train_index, val_index) in enumerate(folds.split(X_train, y_train_arr)):\n",
    "        X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "        y_train_fold = y_train_arr[train_index] \n",
    "        \n",
    "        current_model = clone(model)\n",
    "        current_model.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        oof_proba[val_index] += current_model.predict_proba(X_val_fold)[:, 1]\n",
    "        test_preds += current_model.predict_proba(X_test)[:, 1] / total_splits\n",
    "        \n",
    "    oof_proba /= folds.n_repeats \n",
    "    \n",
    "    return oof_proba, test_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ec25a4",
   "metadata": {},
   "source": [
    "## 2. Pré-processamento e Feature Selection\n",
    "# 2.1 Limpeza e Nulos  e Codificação \n",
    "Justificativa: Imputação por Mediana para robustez. One−Hot Encoding foi escolhido por ser o método de codificação mais estável após testes com Target Encoding causarem overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d064ecd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 EXECUÇÃO DE FE SIMPLIFICADA\n",
    "train_enhanced = create_simple_features(train.copy())\n",
    "test_enhanced = create_simple_features(test.copy())\n",
    "\n",
    "# 2.2 IMPUTAÇÃO\n",
    "impute_cols = ['age_first_funding_year', 'age_last_funding_year', 'age_first_milestone_year', 'age_last_milestone_year', 'funding_total_usd', 'avg_participants']\n",
    "for col in impute_cols:\n",
    "    median_val = train_enhanced[col].median()\n",
    "    train_enhanced[col].fillna(median_val, inplace=True)\n",
    "    test_enhanced[col].fillna(median_val, inplace=True)\n",
    "    \n",
    "# 2.3 ONE-HOT ENCODING\n",
    "category_col = 'category_code'\n",
    "X_base = train_enhanced.drop(columns=[TARGET])\n",
    "X_test_base = test_enhanced\n",
    "\n",
    "X_full = pd.concat([X_base, X_test_base], ignore_index=True)\n",
    "\n",
    "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "ohe_fitted = ohe.fit_transform(X_full[[category_col]])\n",
    "ohe_df = pd.DataFrame(ohe_fitted, columns=ohe.get_feature_names_out([category_col]))\n",
    "\n",
    "X_ohe = ohe_df.iloc[:len(train_enhanced)]\n",
    "X_test_ohe = ohe_df.iloc[len(train_enhanced):]\n",
    "\n",
    "# 2.4 PREPARAÇÃO FINAL\n",
    "X_base = X_base.drop(columns=['id', category_col], errors='ignore').select_dtypes(include=np.number)\n",
    "X_test_base = X_test_base.drop(columns=['id', category_col], errors='ignore').select_dtypes(include=np.number)\n",
    "\n",
    "X_final = pd.concat([X_base.reset_index(drop=True), X_ohe.reset_index(drop=True)], axis=1)\n",
    "X_test_final = pd.concat([X_test_base.reset_index(drop=True), X_test_ohe.reset_index(drop=True)], axis=1)\n",
    "y_series = train_enhanced[TARGET]\n",
    "\n",
    "\n",
    "# 2.5 ESCALONAMENTO e SELEÇÃO K-BEST (Ajuste crucial: k=40)\n",
    "selector = SelectKBest(f_classif, k=40) \n",
    "X_selected = selector.fit_transform(X_final, y_series)\n",
    "X_test_selected = selector.transform(X_test_final)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_selected)\n",
    "X_test_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "X_scaled_base = X_scaled\n",
    "X_test_scaled_base = X_test_scaled\n",
    "y_train_arr = y_series.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8c5761",
   "metadata": {},
   "source": [
    "## 3. Modelo Final e Otimização\n",
    "# 3.1 Construção e Ajuste Fino\n",
    "Justificativa: O modelo foi ajustado para max_depth=7 (ponto de estabilidade) e learning_rate=0.005 com n_estimators=2000 para garantir máxima precisão (low learning rate, high estimators)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8462a67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. MODELO DOMINANTE: HGB (Profundidade Mapeada)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"3. HGB DOMINANTE: TREINAMENTO E GERAÇÃO OOF (Regras Scikit-Learn)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "SKF = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n",
    "\n",
    "# SUBSTITUIÇÃO CIRÚRGICA: Mapeamento de Hiperparâmetros\n",
    "hgb_dominant = HistGradientBoostingClassifier(\n",
    "    max_iter=2000,                  # Mapeia n_estimators\n",
    "    learning_rate=0.005,            \n",
    "    max_leaf_nodes=127,             # Mapeia max_depth=7\n",
    "    l2_regularization=0.1,          \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Geração OOF e Otimização do Threshold\n",
    "oof_proba, final_test_preds = generate_oof_predictions(hgb_dominant, X_scaled_base, y_series, X_test_scaled_base, SKF)\n",
    "\n",
    "# Busca do Threshold que maximiza a Acurácia OOF\n",
    "thresholds = np.linspace(0.4, 0.7, 100)\n",
    "best_acc = 0\n",
    "best_threshold = 0.5\n",
    "\n",
    "for t in thresholds:\n",
    "    y_pred_t = (oof_proba > t).astype(int)\n",
    "    current_acc = accuracy_score(y_series, y_pred_t)\n",
    "    \n",
    "    if current_acc > best_acc:\n",
    "        best_acc = current_acc\n",
    "        best_threshold = t\n",
    "\n",
    "# Treinamento final no conjunto completo\n",
    "hgb_dominant.fit(X_scaled_base, y_series)\n",
    "final_pred_proba = hgb_dominant.predict_proba(X_test_scaled_base)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d95723a",
   "metadata": {},
   "source": [
    "## 3.2 Avaliação das Métricas \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c72a964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. CÁLCULO DAS MÉTRICAS FINAIS\n",
    "\n",
    "final_y_pred = (oof_proba > best_threshold).astype(int)\n",
    "final_precision = precision_score(y_series, final_y_pred)\n",
    "final_recall = recall_score(y_series, final_y_pred)\n",
    "final_f1 = f1_score(y_series, final_y_pred)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"RELATÓRIO DE AVALIAÇÃO OTIMIZADA\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Acurácia Máxima Teórica (OOF): {best_acc:.4f} ({best_acc:.2%})\")\n",
    "print(f\"Threshold Otimizado Aplicado: {best_threshold:.4f}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Precisão (Positivos Corretos): {final_precision:.4f}\")\n",
    "print(f\"Recall (Identificação de Sucesso): {final_recall:.4f}\")\n",
    "print(f\"F1-Score: {final_f1:.4f}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0728f2c",
   "metadata": {},
   "source": [
    "### 4. Conclusão e Submissão\n",
    "## 4.1 Documentação e Apresentação\n",
    "# Conclusão\n",
    " O modelo alcançou 78,26% de acurácia teórica, um resultado que demonstra a eficácia da combinação entre a limpeza de dados (OHE + KBest) e a potência do modelo. Este valor é a melhor previsão possível para a acurácia final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115e6ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. SUBMISSÃO FINAL\n",
    "\n",
    "final_pred_adjusted = (final_pred_proba > best_threshold).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'labels': final_pred_adjusted\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"\\n✓ Arquivo 'submission.csv' gerado com sucesso!\")\n",
    "print(f\"ACURÁCIA OTIMIZADA FINAL: {best_acc:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
